{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
{
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FranciscoBPereira/MEI-AID/blob/main/MEI_AID_ex1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aofNbODMK2M"
      },
      "outputs": [],
      "source": [
        "# Setup, Version check and Common imports\n",
        "\n",
        "# Python ≥3.10 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 10)\n",
        "\n",
        "\n",
        "# TensorFlow ≥2.10 is required\n",
        "import tensorflow as tf\n",
        "assert tf.__version__ >= \"2.10\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load IMDB dataset from keras datasets: https://keras.io/api/datasets/imdb/\n",
        "# Information is preprocessed and ready to use\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "max_features = 10000    # Only the most common max_feature words are kept\n",
        "common_words = 10       # Skips the top common_words most common words\n",
        "\n",
        "# The load_data() method creates train and test sets.\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=max_features, skip_top=common_words)\n",
        "\n",
        "# It retrieves a dict mapping words to their index in the IMDB dataset.\n",
        "word_index = keras.datasets.imdb.get_word_index()"
      ],
      "metadata": {
        "id": "VDzVsTu0H1nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization of a few reviews, both encoded and as a raw text\n",
        "# Labels: 0(Bad), 1(Good)\n",
        "\n",
        "# Choose a review\n",
        "review = 0\n",
        "\n",
        "print(\"Review Length: \" ,len(x_train[review]))\n",
        "print(x_train[review])\n",
        "\n",
        "tam = len(x_train[review])\n",
        "print('Label ', y_train[review])\n",
        "\n",
        "\n",
        "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
        "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
        "    id_to_word[id_] = token\n",
        "\" \".join([id_to_word[id_] for id_ in x_train[review][:tam]])"
      ],
      "metadata": {
        "id": "4MJmyBBiIsDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cut reviews to enhance efficiency (by default, words are cut at the beginning)\n",
        "\n",
        "# Sentiment analysis predictions will be made just considering the last words of the review\n",
        "#https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
        "\n",
        "maxlen = 100\n",
        "\n",
        "x_trainP = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_testP = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
      ],
      "metadata": {
        "id": "OarDWnyvJC5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization of a few reviews, after the cut\n",
        "\n",
        "# Choose a review\n",
        "\n",
        "review = 0\n",
        "tam = len(x_trainP[review])\n",
        "\n",
        "print('Length ', tam)\n",
        "print('Label ', y_train[review])\n",
        "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
        "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
        "    id_to_word[id_] = token\n",
        "\" \".join([id_to_word[id_] for id_ in x_trainP[review][:tam]])\n"
      ],
      "metadata": {
        "id": "unnBbUx3Jhph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### MODEL A\n",
        "\n",
        "# A straightforward feedforward neural network\n",
        "\n",
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "modelA = keras.Sequential([\n",
        "    layers.Flatten(input_shape=[maxlen, 1]),\n",
        "    layers.Dense(20),\n",
        "    layers.Dense(20),\n",
        "    layers.Dense(1, activation=\"sigmoid\")\n",
        "])"
      ],
      "metadata": {
        "id": "dMsApNK1Jt_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelA.summary()"
      ],
      "metadata": {
        "id": "X5O5wwwxQyHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilation and Training\n",
        "\n",
        "modelA.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "historyA = modelA.fit(x_trainP, y_train, epochs=10, validation_split=0.2)"
      ],
      "metadata": {
        "id": "JGaoAiqtQ4Ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ModelA Performance on the Test set\n",
        "\n",
        "modelA.evaluate(x_testP, y_test)"
      ],
      "metadata": {
        "id": "2VNGR21SRCNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the evolution of the accuracy metrics\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "x = pd.DataFrame(historyA.history, columns = ['accuracy', 'val_accuracy'])\n",
        "x.plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cCl5KM_HRPgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### MODEL B\n",
        "\n",
        "# Add a raw (untrained) embedding layer\n",
        "# https://www.tensorflow.org/tutorials/text/word_embeddings\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\n",
        "\n",
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Embedding dimension\n",
        "output_emb = 20\n",
        "\n",
        "modelB = keras.Sequential([\n",
        "    keras.Input(shape=[maxlen]),\n",
        "    layers.Embedding(max_features,  output_emb),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(20),\n",
        "    layers.Dense(20),\n",
        "    layers.Dense(1, activation=\"sigmoid\")\n",
        "])"
      ],
      "metadata": {
        "id": "JHJ-OQg1RPra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelB.summary()"
      ],
      "metadata": {
        "id": "Z3dSBbKRbv_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilation and Training\n",
        "\n",
        "modelB.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "historyB = modelB.fit(x_trainP, y_train, epochs=10, validation_split=0.2)"
      ],
      "metadata": {
        "id": "81aNxB8AYSwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ModelB Performance on the Test set\n",
        "\n",
        "modelB.evaluate(x_testP, y_test)"
      ],
      "metadata": {
        "id": "SpuVG7kIZP3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the evolution of the accuracy metrics\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "x = pd.DataFrame(historyB.history, columns = ['accuracy', 'val_accuracy'])\n",
        "x.plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lXCZ5ytKZQGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### MODEL C\n",
        "\n",
        "# Replace the feedforward architecture by a recurrent neural network with LSTM cells\n",
        "\n",
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "\n",
        "# Embedding dimension\n",
        "output_emb = 20\n",
        "\n",
        "modelC = keras.Sequential([\n",
        "    keras.Input(shape=[maxlen]),\n",
        "    layers.Embedding(max_features,  output_emb),\n",
        "    layers.SimpleRNN(20, return_sequences=True),\n",
        "    layers.SimpleRNN(20),\n",
        "    layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n"
      ],
      "metadata": {
        "id": "tF_d6uS9Zc7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelC.summary()"
      ],
      "metadata": {
        "id": "LemiHrwCbHmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilation and Training\n",
        "\n",
        "modelC.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "historyC = modelC.fit(x_trainP, y_train, epochs=10, validation_split=0.2)"
      ],
      "metadata": {
        "id": "dQ9_Iv6tbHpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ModelC Performance on the Test set\n",
        "\n",
        "modelC.evaluate(x_testP, y_test)"
      ],
      "metadata": {
        "id": "wx7XNLZkbHre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the evolution of the accuracy metrics\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "x = pd.DataFrame(historyC.history, columns = ['accuracy', 'val_accuracy'])\n",
        "x.plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uRNM-V_mb_Re"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
